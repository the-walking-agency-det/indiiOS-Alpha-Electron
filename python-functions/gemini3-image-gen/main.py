import base64
import os
import functions_framework
from flask import jsonify, make_response
from google import genai
from google.genai import types

# --- Configuration ---
PROJECT_ID = os.environ.get("GOOGLE_CLOUD_PROJECT", "indiios-v-1-1")
LOCATION = "us-central1"
MODEL_ID = "gemini-3-pro-image-preview"

# Initialize the Gen AI Client for Vertex AI
client = genai.Client(
    vertexai=True,
    project=PROJECT_ID,
    location=LOCATION
)

@functions_framework.http
def generate_image_gemini3(request):
    """
    HTTP Cloud Function for Gemini 3 Image Generation.
    Args:
        request (flask.Request): JSON body with 'prompt'.
    Returns:
        Base64 image or error.
    """
    # 1. Parse Input
    request_json = request.get_json(silent=True)
    if not request_json or 'prompt' not in request_json:
        return make_response(jsonify({"error": "Missing prompt"}), 400)
    
    prompt = request_json['prompt']
    
    # 2. Configure Generation
    # GEMINI 3 CRITICAL: Temperature MUST be 1.0 to ensure reasoning performance.
    config = types.GenerateContentConfig(
        temperature=1.0, 
        response_modalities=['TEXT', 'IMAGE'],
        # Use MEDIA_RESOLUTION_HIGH (1120 tokens) or ULTRA_HIGH (2240 tokens)
        media_resolution='MEDIA_RESOLUTION_HIGH',
    )

    try:
        # 3. Call Gemini 3
        # We capture the full response to handle thinking tokens and signatures
        response = client.models.generate_content(
            model=MODEL_ID,
            contents=[prompt],
            config=config
        )

        # 4. Handle Thought Signature
        # For multi-turn editing, the 'thought_signature' is mandatory.
        # It links the model's reasoning across consecutive turns.
        thought_sig = getattr(response, 'thought_signature', None)
        
        # 5. Process Output
        # The model returns a list of parts, we look for the IMAGE part.
        base64_image = None
        for part in response.candidates[0].content.parts:
            if part.inline_data:
                base64_image = base64.b64encode(part.inline_data.data).decode('utf-8')
                break
        
        if not base64_image:
            return make_response(jsonify({
                "error": "No image generated by model",
                "reasoning": response.text if response.text else "N/A"
            }), 500)

        # 6. Return Response
        return jsonify({
            "status": "success",
            "image": base64_image,
            "mimeType": "image/png",
            "thoughtSignature": thought_sig, 
            "usageMetadata": {
                "promptTokenCount": response.usage_metadata.prompt_token_count,
                "candidatesTokenCount": response.usage_metadata.candidates_token_count,
                "totalTokenCount": response.usage_metadata.total_token_count
            }
        })

    except Exception as e:
        print(f"Error calling Gemini 3: {e}")
        return make_response(jsonify({"error": str(e)}), 500)
