import { GoogleGenAI } from '@google/genai';

// Helper to clean JSON (remove markdown blocks)
function cleanJSON(text: string): string {
    return text.replace(/```json\n|\n```/g, '').replace(/```/g, '').trim();
}

class AIService {
    private apiKey: string;
    private projectId?: string;
    private location?: string;
    private useVertex: boolean;

    constructor() {
        // Support both Vite env and legacy process.env if needed
        this.apiKey = import.meta.env.VITE_API_KEY || (window as any).process?.env?.API_KEY || '';
        this.projectId = import.meta.env.VITE_VERTEX_PROJECT_ID;
        this.location = import.meta.env.VITE_VERTEX_LOCATION || 'us-central1';
        this.useVertex = import.meta.env.VITE_USE_VERTEX === 'true';

        if (!this.apiKey && !this.projectId) {
            console.warn("Missing VITE_API_KEY or VITE_VERTEX_PROJECT_ID");
        }
    }

    private getClient() {
        if (this.useVertex && this.projectId) {
            // Vertex AI Client
            console.log("Using Vertex AI with Project:", this.projectId);
            return new GoogleGenAI({
                vertexai: true,
                project: this.projectId,
                location: this.location || 'us-central1',
                apiKey: this.apiKey
            });
        }

        if (!this.apiKey) throw new Error("API Key not found. Please set VITE_API_KEY in .env");
        // Veo 3.1 is likely in v1alpha
        return new GoogleGenAI({ apiKey: this.apiKey, apiVersion: 'v1alpha' });
    }

    async generateContent(options: {
        model: string;
        contents: any;
        config?: any;
        systemInstruction?: string;
    }) {
        const ai = this.getClient();
        const config = options.config || {};
        if (options.systemInstruction) config.systemInstruction = options.systemInstruction;

        return await ai.models.generateContent({
            model: options.model,
            contents: options.contents,
            config: config
        });
    }

    async embedContent(options: {
        model: string;
        content: any;
    }) {
        const ai = this.getClient();
        return await ai.models.embedContent({
            model: options.model,
            contents: options.content
        });
    }

    async generateVideo(options: {
        model: string;
        prompt: string;
        image?: { imageBytes: string; mimeType: string };
        config?: any;
    }) {
        // Custom implementation for Veo on Vertex AI using raw REST API + Access Token
        // This bypasses SDK limitations in the browser.

        try {
            // 1. Get Access Token (generated by gcloud to public/access_token.txt)
            const tokenRes = await fetch('/access_token.txt');
            if (!tokenRes.ok) throw new Error("Could not load access token. Please run: gcloud auth print-access-token > public/access_token.txt");
            const accessToken = (await tokenRes.text()).trim();

            if (!this.projectId) throw new Error("VITE_VERTEX_PROJECT_ID is missing in .env");

            // 2. Construct Request
            const endpoint = `https://us-central1-aiplatform.googleapis.com/v1/projects/${this.projectId}/locations/us-central1/publishers/google/models/${options.model}:predict`;

            // Veo expects specific instance structure
            const instance: any = {
                prompt: options.prompt
            };

            if (options.image) {
                // Try inline bytes. If this fails, we might need GCS.
                // Vertex AI often uses "image": { "bytesBase64Encoded": ... } or similar.
                // For Veo specifically, let's try the standard Vertex structure.
                instance.image = {
                    imageBytes: options.image.imageBytes, // SDK provided base64
                    mimeType: options.image.mimeType
                };
            }

            if (options.config?.lastFrame) {
                instance.lastFrame = {
                    imageBytes: options.config.lastFrame.imageBytes,
                    mimeType: options.config.lastFrame.mimeType
                };
            }

            const body = {
                instances: [instance],
                parameters: {
                    sampleCount: 1,
                    // Map other config params if needed
                }
            };

            // 3. Call API
            const res = await fetch(endpoint, {
                method: 'POST',
                headers: {
                    'Authorization': `Bearer ${accessToken}`,
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify(body)
            });

            if (!res.ok) {
                const errText = await res.text();
                throw new Error(`Vertex API Error ${res.status}: ${errText}`);
            }

            const data = await res.json();

            // 4. Handle Response
            // Veo usually returns a video URI in the prediction
            // data.predictions[0] might be the URI or an object
            const prediction = data.predictions?.[0];
            if (!prediction) throw new Error("No prediction returned");

            // If it returns a GCS URI (gs://), we can't display it directly.
            // We'd need to sign it or proxy it.
            // But let's see what it returns.
            // Sometimes it returns bytes?

            // For now, return the raw result as text so ImageService can try to parse it.
            return JSON.stringify(prediction);

        } catch (e) {
            console.error("Video Gen Error", e);
            throw e;
        }
    }

    parseJSON(text: string | undefined) {
        if (!text) return {};
        try {
            return JSON.parse(cleanJSON(text));
        } catch (e) {
            console.error("Failed to parse JSON:", text);
            return {};
        }
    }
}

export const AI = new AIService();
